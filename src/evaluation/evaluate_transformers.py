"""
This module evaluates transformer-based models for tasks like content moderation,
rephrasing refinement, and sentiment analysis using Hugging Face's `transformers` library.
"""

import os
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from dotenv import load_dotenv
from src.tasks import text_summarization
load_dotenv(dotenv_path=".env", override=True, verbose=True)


class TransformersModel:
    """
    A class for handling transformer-based models for text generation tasks.
    """

    def __init__(self, model_id: str, temp: float, print_logs=False) -> None:
        """
        Initializes the evaluation class for transformer models.
        Args:
            model_id (str): The identifier for the pre-trained transformer model.
            temp (float): The temperature parameter for controlling randomness in model outputs.
            print_logs (bool, optional): Flag to enable or disable logging. Defaults to False.
        Attributes:
            model_id (str): The identifier for the pre-trained transformer model.
            temp (float): The temperature parameter for controlling randomness in model outputs.
            print_logs (bool): Flag to enable or disable logging.
            model (AutoModelForCausalLM): The loaded transformer model for causal language modeling.
            tokenizer (AutoTokenizer): The tokenizer associated with the transformer model.
        """

        self.model_id = model_id
        self.temp = temp
        self.print_logs = print_logs
        print(f"[INFO] Initializing Model {self.model_id}...")
        self.model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=self.model_id,
            cache_dir=r"models/cache",
            device_map="auto",
            force_download=False,
        )

        print("[INFO] Initializing Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=self.model_id,
            cache_dir=r"models/cache",
            force_download=False,
        )

    def get_chat_template(self, message: list):
        """
        Applies a chat template to the given conversation message.
        This method uses the tokenizer to format the conversation message
        according to a predefined chat template. Optionally, it adds a
        generation prompt to the conversation.
        Args:
            message (list): A list of dictionaries representing the conversation.
                    Each dictionary contains a `role` (e.g., "system", "user")
                    and `content` (the text associated with the role).
        Returns:
            str: The formatted conversation message as a string.
        """

        if self.print_logs:
            print("[INFO] Applying Chat Template...")
        return self.tokenizer.apply_chat_template(
            conversation=message,
            add_generation_prompt=True,
            tokenize=False,
        )

    def get_input_tokens(self, text):
        """
        Converts the input text into tokenized format suitable for transformer models.
        Args:
            text (str): The input text to be tokenized.
        Returns:
            torch.Tensor: A tensor containing the tokenized representation of the input text,
            prepared for processing by transformer models. The tensor is moved to the device
            specified by the "DEVICE" environment variable.
        Raises:
            TypeError: If the tokenizer fails to process the input text.
        """

        if self.print_logs:
            print("[INFO] Getting Input Tokens...")
        return self.tokenizer(
            text=[text],
            return_tensors="pt",
            padding=True,
        ).to(
            os.getenv("DEVICE")
        )  # type: ignore

    def get_output_tokens(self, input_tokens):
        """
        Generate output tokens from input tokens using the transformer model.
        Args:
            input_tokens (BatchEncoding): A batch of input tokens containing `input_ids`
                and `attention_mask` attributes.
        Returns:
            List[List[int]]: A list of output token sequences, where each sequence is
            generated by the model and excludes the input tokens.
        """

        if self.print_logs:
            print("[INFO] Generating Ouput Tokens...")
        generated_ids = self.model.generate(
            inputs=input_tokens.input_ids,
            attention_mask=input_tokens.attention_mask,
            max_new_tokens=2048,
            # max_time=10.0,
            use_cache=True,
            temperature=1.0,
            do_sample=True,
            top_k=50,
            top_p=1,
        )

        return [
            output_ids[len(input_ids) :]
            for input_ids, output_ids in zip(input_tokens.input_ids, generated_ids)
        ]

    def get_response(self, output_tokens):
        """
        Decodes the output tokens generated by the model into a human-readable response.
        Args:
            output_tokens (list): A list of token sequences generated by the model.
        Returns:
            str: The decoded response string.
        """

        if self.print_logs:
            print("[INFO] Generating Response...")
        return self.tokenizer.batch_decode(
            sequences=output_tokens, skip_special_tokens=True
        )[0]

    def create(self, message):
        """
        Processes the given message through a series of transformations and returns a response.
        Args:
            message (str): The input message to be processed.
        Returns:
            Any: The final response generated after processing the input message.
        """

        return self.get_response(
            self.get_output_tokens(
                self.get_input_tokens(self.get_chat_template(message))
            )
        )



def test_model():
    """
    Test function to evaluate the TransformersModel class.
    This function initializes a model and tests it with a sample message.
    It prints the response generated by the model.
    """

    print("\n[INFO] Testing TransformersModel...\n")
    model = TransformersModel(model_id=os.getenv("QWEN_ID"), temp=0.2, print_logs=True)

    with open(
        file=r'src/evaluation/text.json',
        mode='r',
        encoding='utf-8'
    ) as f:
        file_content = json.load(f)

    text = file_content[0]['text']
    print(f"[INFO] Input Text: {text}")

    message = text_summarization.get_message(text)
    response = model.create(message)
    print(f"\n[RESULT]:\n{response}")

def main():
    """main function to run the test_model function."""
    test_model()


#     model = TransformersModel(model_id=args.model, temp=args.temp)
#     text = model.get_chat_template(message)  # type: ignore
#     input_tokens = model.get_input_tokens(text)
#     output_tokens = model.get_output_tokens(input_tokens)
#     response = model.get_response(output_tokens)

#     print(f"\n[RESULT]:\n{response}")


if __name__ == "__main__":
    main()
# To run this file : python -m src.evaluation.evaluate_transformers
