"""
This module evaluates transformer-based models for tasks like content moderation,
rephrasing refinement, and sentiment analysis using Hugging Face's `transformers` library.
"""

import os
import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from dotenv import load_dotenv
from src.tasks import text_summarization
load_dotenv(dotenv_path=".env", override=True, verbose=True)


class TransformersModel:
    """
    A class for handling transformer-based models for text generation tasks.
    """

    def __init__(self, model_id: str, temp: float, print_logs=False) -> None:
        """
        Initializes the evaluation class for transformer models.
        Args:
            model_id (str): The identifier for the pre-trained transformer model.
            temp (float): The temperature parameter for controlling randomness in model outputs.
            print_logs (bool, optional): Flag to enable or disable logging. Defaults to False.
        Attributes:
            model_id (str): The identifier for the pre-trained transformer model.
            temp (float): The temperature parameter for controlling randomness in model outputs.
            print_logs (bool): Flag to enable or disable logging.
            model (AutoModelForCausalLM): The loaded transformer model for causal language modeling.
            tokenizer (AutoTokenizer): The tokenizer associated with the transformer model.
        """

        self.model_id = model_id
        self.temp = temp
        self.print_logs = print_logs
        self.mask = None
        print(f"[INFO] Initializing Model {self.model_id}...")
        self.model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=self.model_id,
            cache_dir=r"models/cache",
            device_map="auto",
            force_download=False,
        )

        print("[INFO] Initializing Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=self.model_id,
            cache_dir=r"models/cache",
            force_download=False,
        )

    def get_chat_template(self, message: list):
        """
        Applies a chat template to the given conversation message.
        This method uses the tokenizer to format the conversation message
        according to a predefined chat template. Optionally, it adds a
        generation prompt to the conversation.
        Args:
            message (list): A list of dictionaries representing the conversation.
                    Each dictionary contains a `role` (e.g., "system", "user")
                    and `content` (the text associated with the role).
        Returns:
            str: The formatted conversation message as a string.
        """

        if self.print_logs:
            print("[INFO] Applying Chat Template...")
        return self.tokenizer.apply_chat_template(
            conversation=message,
            add_generation_prompt=True,
            tokenize=False,
        )

    def get_input_tokens(self, text):
        """
        Converts the input text into tokenized format suitable for transformer models.
        Args:
            text (str): The input text to be tokenized.
        Returns:
            torch.Tensor: A tensor containing the tokenized representation of the input text,
            prepared for processing by transformer models. The tensor is moved to the device
            specified by the "DEVICE" environment variable.
        Raises:
            TypeError: If the tokenizer fails to process the input text.
        """

        if self.print_logs:
            print("[INFO] Getting Input Tokens...")
        return self.tokenizer(
            text=[text],
            return_tensors="pt",
            padding=True,
        ).to(
            os.getenv("DEVICE")
        )  # type: ignore

    def get_output_tokens(self, input_tokens):
        """
        Generate output tokens from input tokens using the transformer model.
        Args:
            input_tokens (BatchEncoding): A batch of input tokens containing `input_ids`
                and `attention_mask` attributes.
        Returns:
            List[List[int]]: A list of output token sequences, where each sequence is
            generated by the model and excludes the input tokens.
        """

        def logits_processor(token_ids, logits):
            """A processor to ban Chinese character"""
            if self.mask is None:
                # as we don't know where the Chinses tokens locate at which index
                # in the vocabulary but we know how it looks like and the range of it

                # decode all the tokens in the vocabulary in order
                token_ids = torch.arange(logits.size(-1))
                decoded_tokens = self.tokenizer.batch_decode(
                    token_ids.unsqueeze(1), skip_special_tokens=True
                )

                # create a mask tensor to exclude positions of Chinese characters.
                # since this process uses a for loop and is time-consuming,
                # the result will be stored as a property for later use to ensure it only runs once.
                self.mask = torch.tensor(
                    [
                        # loop through each token in the vocabulary and compare it to Chinese characters.
                        any(
                            0x4E00 <= ord(c) <= 0x9FFF
                            or 0x3400 <= ord(c) <= 0x4DBF
                            or 0xF900 <= ord(c) <= 0xFAFF
                            for c in token
                        )
                        for token in decoded_tokens
                    ]
                )

            # mask the score by - inf
            logits[:, self.mask] = -float("inf")
            return logits

        if self.print_logs:
            print("[INFO] Generating Ouput Tokens...")

        generated_ids = self.model.generate(
            **input_tokens,
            max_new_tokens=2000,
            temperature=0.1,
            # add the logits_processor here
            logits_processor=[logits_processor],
        )
        generated_ids = [
            output_ids[len(input_ids) :]
            for input_ids, output_ids in zip(input_tokens.input_ids, generated_ids)
        ]

        return generated_ids


    # def get_output_tokens(self, input_tokens):
    #     """
    #     Generate output tokens from input tokens using the transformer model.
    #     Args:
    #         input_tokens (BatchEncoding): A batch of input tokens containing `input_ids`
    #             and `attention_mask` attributes.
    #     Returns:
    #         List[List[int]]: A list of output token sequences, where each sequence is
    #         generated by the model and excludes the input tokens.
    #     """

    #     if self.print_logs:
    #         print("[INFO] Generating Ouput Tokens...")
    #     generated_ids = self.model.generate(
    #         inputs=input_tokens.input_ids,
    #         attention_mask=input_tokens.attention_mask,
    #         max_new_tokens=2048,
    #         # max_time=10.0,
    #         use_cache=True,
    #         temperature=1.0,
    #         do_sample=True,
    #         top_k=50,
    #         top_p=1,
    #     )

        # return [
        #     output_ids[len(input_ids) :]
        #     for input_ids, output_ids in zip(input_tokens.input_ids, generated_ids)
        # ]

    def get_response(self, output_tokens):
        """
        Decodes the output tokens generated by the model into a human-readable response.
        Args:
            output_tokens (list): A list of token sequences generated by the model.
        Returns:
            str: The decoded response string.
        """

        if self.print_logs:
            print("[INFO] Generating Response...")
        return self.tokenizer.batch_decode(
            sequences=output_tokens, skip_special_tokens=True
        )[0]
    
    def load_adapter(self, adapter_id: str):
        """
        Loads an adapter into the model.
        Args:
            adapter_id (str): The identifier of the adapter to be loaded.
                      This could be a path to the adapter file or a predefined adapter name.
        Returns:
            None
        """

        self.model.load_adapter(adapter_id)

    def create(self, message):
        """
        Processes the given message through a series of transformations and returns a response.
        Args:
            message (str): The input message to be processed.
        Returns:
            Any: The final response generated after processing the input message.
        """

        return self.get_response(
            self.get_output_tokens(
                self.get_input_tokens(self.get_chat_template(message))
            )
        )



def test_model():
    """
    Test function to evaluate the TransformersModel class.
    This function initializes a model and tests it with a sample message.
    It prints the response generated by the model.
    """

    print("\n[INFO] Testing TransformersModel...\n")
    model_id = os.getenv("QWEN_ID")
    id = "/teamspace/studios/this_studio/Text-Summarization/models/finetuned"

    model = TransformersModel(model_id=model_id, temp=0.2, print_logs=True)
    model.load_adapter(id)

    with open(
        file=r'src/evaluation/text.json',
        mode='r',
        encoding='utf-8'
    ) as f:
        file_content = json.load(f)

    text = file_content[0]['text']
    print(f"[INFO] Input Text: {text}")

    message = text_summarization.get_message(text)
    response = model.create(message)
    print(f"\n[RESULT]:\n{response}")

def main():
    """main function to run the test_model function."""
    test_model()


#     model = TransformersModel(model_id=args.model, temp=args.temp)
#     text = model.get_chat_template(message)  # type: ignore
#     input_tokens = model.get_input_tokens(text)
#     output_tokens = model.get_output_tokens(input_tokens)
#     response = model.get_response(output_tokens)

#     print(f"\n[RESULT]:\n{response}")


if __name__ == "__main__":
    main()
# To run this file : python -m src.evaluation.evaluate_transformers
